/*
Copyright 2022 The Katalyst Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package state

import (
	"fmt"

	"k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/klog/v2"

	"github.com/kubewharf/katalyst-api/pkg/consts"
	advisorapi "github.com/kubewharf/katalyst-core/pkg/agent/qrm-plugins/cpu/dynamicpolicy/cpuadvisor"
	"github.com/kubewharf/katalyst-core/pkg/util/general"
	"github.com/kubewharf/katalyst-core/pkg/util/machine"
)

type ListAndWatchResponseValidator func(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error

const (
	PoolNameShare     = "share"
	PoolNameReclaim   = "reclaim"
	PoolNameDedicated = "dedicated"
	PoolNameReserve   = "reserve"
	// PoolNameFallback is not a real pool and is the union result of
	// all online pools to put pod should have been isolated
	PoolNameFallback = "fallback"
)

var (
	// StaticPools are generated by cpu plugin statically,
	// and they will be ignored when reading cpu advisor list and watch response
	StaticPools = sets.NewString(
		PoolNameReserve,
	)

	// ResidentPools are guaranteed existing in state
	ResidentPools = sets.NewString(
		PoolNameReclaim,
	).Union(StaticPools)
)

var GetContainerRequestedCores func(allocationInfo *AllocationInfo) int

// [TODO]: valida all shared_cores and reclaimed_cores haven't numa results
var listAndWatchResponseValidators = []ListAndWatchResponseValidator{
	validateStaticPools,
	validateDedicatedEntries,
}

// GetPoolsQuantityMapFromCPUAdvisorResp GetQuantityMap returns a map from cpu-set pool to cpu size;
// this info is constructed by advisor response, and this response contain cpu-set pool
// by a special container name .i.e empty string
func GetPoolsQuantityMapFromCPUAdvisorResp(resp *advisorapi.ListAndWatchResponse) (map[string]int, error) {
	ret := make(map[string]int)

	if resp == nil {
		return ret, nil
	}

	for poolName, entry := range resp.Entries {
		calculationEntries := entry.Entries

		// only deal with pool
		if len(calculationEntries) != 1 || calculationEntries[""] == nil {
			continue
		}

		calculationInfo := calculationEntries[""]

		// todo: currently sys advisor only returns calculation results
		//  for pools which are npt overlapped with others and isn't NUMA aware.
		// 	and we should refine judgement here when sys advisor supports NUMA
		// 	aware pool results overlapped with others.
		if calculationInfo == nil ||
			len(calculationInfo.CalculationResultsByNumas) != 1 ||
			calculationInfo.CalculationResultsByNumas[-1] == nil ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].OverlapTargets) != 0 ||
			calculationInfo.OwnerPoolName != poolName {
			return nil, fmt.Errorf("invalid calculation result: %s for pool: %s", calculationInfo.String(), poolName)
		}

		if StaticPools.Has(poolName) {
			klog.Infof("[GetPoolsQuantityMapFromCPUAdvisorResp] skip static pool: %s in cpu advisor resp", poolName)
			continue
		}

		ret[poolName] = int(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].Result)
	}
	return ret, nil
}

// GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp returns a map from cpu-set pool to cpu size;
// this info is constructed by combine advisor response and given pod entry list
func GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) (map[string]int, error) {
	entriesQuantityMap := machine.GetQuantityMap(entries.GetPoolsCPUset(ResidentPools))

	qosAwarePoolsQuantityMap, err := GetPoolsQuantityMapFromCPUAdvisorResp(resp)
	if err != nil {
		return nil, fmt.Errorf("GetPoolsQuantityMapFromCPUAdvisorResp failed with error: %v", err)
	}

	poolsQuantityMap := make(map[string]int)
	for poolName, quantity := range entriesQuantityMap {
		poolsQuantityMap[poolName] = quantity

		if _, found := qosAwarePoolsQuantityMap[poolName]; !found {
			klog.Warningf("[GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp] pool: %s isn't found in qos aware resp", poolName)
		}
	}

	// overwrite entriesQuantityMap by qosAwarePoolsQuantityMap
	for poolName, quantity := range qosAwarePoolsQuantityMap {
		if prevQuantity, found := poolsQuantityMap[poolName]; found && prevQuantity != quantity {
			klog.Infof("[GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp] pool: %s overwrite quantity: %d by quantity: %d provided by qos aware",
				poolName, prevQuantity, quantity)
		}
		poolsQuantityMap[poolName] = quantity
	}

	return poolsQuantityMap, nil
}

func GetIsolatedQuantityMapFromPodEntriesAndCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) (map[string]map[string]int, error) {
	dedicatedCoresIsolatedQuantityMap := GetIsolatedQuantityMapFromPodEntries(entries, nil)
	qosAwareIsolatedQuantityMap, err := GetIsolatedQuantityMapFromCPUAdvisorResp(resp, entries)
	if err != nil {
		return nil, fmt.Errorf("GetIsolatedQuantityMapFromCPUAdvisorResp failed with error: %v", err)
	}

	isolatedQuantityMap := make(map[string]map[string]int)
	for podUID, containerEntries := range dedicatedCoresIsolatedQuantityMap {
		if isolatedQuantityMap[podUID] == nil {
			isolatedQuantityMap[podUID] = make(map[string]int)
		}

		for containerName, quantity := range containerEntries {
			isolatedQuantityMap[podUID][containerName] = quantity
		}
	}

	// overwrite dedicatedCoresIsolatedQuantityMap by qosAwareIsolatedQuantityMap
	for podUID, containerEntries := range qosAwareIsolatedQuantityMap {
		if isolatedQuantityMap[podUID] == nil {
			isolatedQuantityMap[podUID] = make(map[string]int)
		}

		for containerName, quantity := range containerEntries {
			if prevQuantity, found := isolatedQuantityMap[podUID][containerName]; found && prevQuantity != quantity {
				klog.Infof("[GetIsolatedQuantityMapFromPodEntriesAndCPUAdvisorResp] pod: %s, container: %s overwrite quantity: %d by quantity: %d provided by qos aware",
					podUID, containerName, prevQuantity, quantity)
			}
			isolatedQuantityMap[podUID][containerName] = quantity
		}
	}

	return isolatedQuantityMap, nil
}

func GetIsolatedQuantityMapFromCPUAdvisorResp(resp *advisorapi.ListAndWatchResponse, chkEntries PodEntries) (map[string]map[string]int, error) {
	ret := make(map[string]map[string]int)

	if resp == nil {
		return ret, nil
	}

	for podUID, entry := range resp.Entries {
		calculationEntries := entry.Entries

		// not deal with pool
		if len(calculationEntries) == 1 && calculationEntries[""] != nil {
			continue
		}

		for containerName, calculationInfo := range calculationEntries {
			if calculationInfo == nil ||
				calculationInfo.OwnerPoolName != PoolNameDedicated {
				continue
			}

			allocationInfo := chkEntries[podUID][containerName]

			if allocationInfo == nil {
				klog.Warningf("[GetIsolatedQuantityMapFromCPUAdvisorResp] qos aware server get entry for pod: %s, container: %s which isn't in checkpoint", podUID, containerName)
				continue
			} else if allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
				continue
			}

			// todo: currently not support per NUMA calculation results for dedicated_cores without numa_binding
			if len(calculationInfo.CalculationResultsByNumas) != 1 ||
				calculationInfo.CalculationResultsByNumas[-1] == nil ||
				len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 {
				return nil, fmt.Errorf("invalid calculation result for pod: %s/%s, container: %s, owner pool: %s",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName, calculationInfo.OwnerPoolName)
			}

			quantity := int(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].Result)

			if quantity == 0 {
				klog.Warningf("[GetIsolatedQuantityMapFromCPUAdvisorResp] dedicated_cores pod: %s/%s container: %s get zero quantity",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName)
				continue
			}

			if ret[podUID] == nil {
				ret[podUID] = make(map[string]int)
			}
			ret[podUID][containerName] = quantity
		}
	}

	return ret, nil
}

func GetIsolatedQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]map[string]int {
	ret := make(map[string]map[string]int)

	for podUID, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for containerName, allocationInfo := range entries {
			// only filter dedicated_cores without numa_binding
			if allocationInfo == nil ||
				// dedicated_cores with numa_binding
				(allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
					allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable) ||
				(allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelDedicatedCores) {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding
			// to pool rather than isolation. calling this function means we will start to adjust allocation,
			// and we will try to isolate those containers, so we will treat them as containers to be isolated.
			var quantity int
			if allocationInfo.OwnerPoolName != PoolNameDedicated {
				quantity = GetContainerRequestedCores(allocationInfo)
			} else {
				quantity = allocationInfo.AllocationResult.Size()
			}

			if quantity == 0 {
				klog.Warningf("[GetIsolatedQuantityMapFromPodEntries] isolated pod: %s/%s container: %s get zero quantity",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName)
				continue
			}

			if ret[podUID] == nil {
				ret[podUID] = make(map[string]int)
			}
			ret[podUID][containerName] = quantity
		}
	}

	return ret
}

// GetPoolsQuantityMapFromPodEntries is used to constrict mapping from
// pool name to its corresponding resource quantity
func GetPoolsQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]int {
	ret := make(map[string]int)

	for _, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for _, allocationInfo := range entries {
			// only count shared_cores not isolated.
			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding to pool rather than isolation.
			// calling this function means we will start to adjust allocation and we will try to isolate those containers,
			// so we will treat them as containers to be isolated.
			if allocationInfo == nil || allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelSharedCores {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			poolName := GetRealOwnerPoolName(allocationInfo)
			if poolName != "" {
				ret[poolName] += GetContainerRequestedCores(allocationInfo)
			}
		}
	}

	return ret
}

// GetSpecifiedPoolName parses the belonging pool name for a given allocation according to QoS level
func GetSpecifiedPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	switch allocationInfo.QoSLevel {
	case consts.PodAnnotationQoSLevelSharedCores:
		specifiedPoolName := GetSpecifiedPoolNameForSharedCores(allocationInfo)
		if specifiedPoolName != "" {
			return specifiedPoolName
		}
		return PoolNameShare
	case consts.PodAnnotationQoSLevelReclaimedCores:
		return PoolNameReclaim
	default:
		return ""
	}
}

// GetRealOwnerPoolName parses the owner pool name for a given allocation
func GetRealOwnerPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.OwnerPoolName
}

// GetSpecifiedPoolNameForSharedCores returns cpu enhancement from allocation
func GetSpecifiedPoolNameForSharedCores(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.Annotations[consts.PodAnnotationCPUEnhancementCPUSet]
}

// GenerateCPUMachineStateByPodEntries is used to re-organize podEntries
// by assembling allocation info into each NUMA node
func GenerateCPUMachineStateByPodEntries(topology *machine.CPUTopology, podEntries PodEntries) (NUMANodeMap, error) {
	if topology == nil {
		return nil, fmt.Errorf("GenerateCPUMachineStateByPodEntries got nil topology")
	}

	machineState := make(NUMANodeMap)
	for _, numaNode := range topology.CPUDetails.NUMANodes().ToSliceInt64() {
		numaNodeState := &NUMANodeState{}

		numaNodeAllCPUs := topology.CPUDetails.CPUsInNUMANodes(int(numaNode)).Clone()
		allocatedCPUsInNumaNode := machine.NewCPUSet()

		for podUID, containerEntries := range podEntries {
			for containerName, allocationInfo := range containerEntries {
				if containerName != "" && allocationInfo != nil {

					// the container hasn't cpuset assignment in the current NUMA node
					if allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)].Size() == 0 &&
						allocationInfo.TopologyAwareAssignments[int(numaNode)].Size() == 0 {
						continue
					}

					// only modify allocated and default properties in NUMA node state for dedicated_cores with NUMA binding
					if allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
						allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
						// only consider original in machine state
						allocatedCPUsInNumaNode = allocatedCPUsInNumaNode.Union(allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)])
					}

					numaNodeAllocationInfo := allocationInfo.Clone()

					topologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs))
					originalTopologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs))

					numaNodeAllocationInfo.AllocationResult = allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.OriginalAllocationResult = allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.TopologyAwareAssignments = topologyAwareAssignments
					numaNodeAllocationInfo.OriginalTopologyAwareAssignments = originalTopologyAwareAssignments

					numaNodeState.SetAllocationInfo(podUID, containerName, numaNodeAllocationInfo)
				}
			}
		}

		numaNodeState.AllocatedCPUSet = allocatedCPUsInNumaNode.Clone()
		numaNodeState.DefaultCPUSet = numaNodeAllCPUs.Difference(numaNodeState.AllocatedCPUSet)

		machineState[int(numaNode)] = numaNodeState
	}

	return machineState, nil
}

func UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp(chkEntries PodEntries, resp *advisorapi.ListAndWatchResponse, poolsQuantityMap map[string]int) error {
	if resp == nil {
		return nil
	}

	for podUID, entry := range resp.Entries {

		calculationEntries := entry.Entries

		// not deal with pool
		if len(calculationEntries) == 1 && calculationEntries[""] != nil {
			continue
		}

		for containerName, calculationInfo := range calculationEntries {
			if calculationInfo == nil ||
				calculationInfo.OwnerPoolName == "" ||
				calculationInfo.OwnerPoolName == PoolNameDedicated {
				continue
			}

			allocationInfo := chkEntries[podUID][containerName]

			if allocationInfo == nil {
				klog.Warningf("[UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp] qos aware server get entry for pod: %s, container: %s which isn't in checkpoint", podUID, containerName)
				continue
			} else if allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelSharedCores && allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelReclaimedCores {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with invalid ownerPoolName: %s",
					podUID, containerName, allocationInfo.QoSLevel, calculationInfo.OwnerPoolName)
			} else if calculationInfo.CalculationResultsByNumas != nil {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with non-empty CalculationResultsByNumas",
					podUID, containerName, allocationInfo.QoSLevel)
			} else if _, found := poolsQuantityMap[calculationInfo.OwnerPoolName]; !found {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with non-empty CalculationResultsByNumas",
					podUID, containerName, allocationInfo.QoSLevel)
			}

			if allocationInfo.OwnerPoolName != calculationInfo.OwnerPoolName {
				klog.Infof("[UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp] qos aware server put pod: %s/%s, container: %s from %s to %s", allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName, allocationInfo.OwnerPoolName, calculationInfo.OwnerPoolName)
			}
			// set allocationInfo.OwnerPoolName as qos aware sever indicates
			allocationInfo.OwnerPoolName = calculationInfo.OwnerPoolName
		}
	}

	return nil
}

func validateDedicatedEntries(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {
	dedicatedAllocationInfos := FilterDedicatedAllocationInfos(entries)
	dedicatedCalculationInfos := FilterDedicatedCalculationInfos(resp)

	if len(dedicatedAllocationInfos) != len(dedicatedCalculationInfos) {
		return fmt.Errorf("dedicatedAllocationInfos length: %d and dedicatedCalculationInfos length: %d mismatch",
			len(dedicatedAllocationInfos), len(dedicatedCalculationInfos))
	}

	for podUID, containerEntries := range dedicatedAllocationInfos {
		for containerName, allocationInfo := range containerEntries {

			calculationInfo := dedicatedCalculationInfos[podUID][containerName]

			if calculationInfo == nil {
				return fmt.Errorf("missing CalculationInfo for pod: %s container: %s", podUID, containerName)
			}

			if allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
				numaCalculationQuantities, err := GetCalculationInfoNUMAQuantities(calculationInfo)

				if err != nil {
					return fmt.Errorf("GetCalculationInfoNUMAQuantities failed with error: %v, pod: %s container: %s",
						err, podUID, containerName)
				}

				// currently we don't support strategy to adjust cpuset of dedicated_cores containers.
				// for stability if the dedicated_cores container calculation result and allocation result,
				// we will return error.

				for numaId, cset := range allocationInfo.TopologyAwareAssignments {
					if cset.Size() != numaCalculationQuantities[numaId] {
						return fmt.Errorf("NUMA: %d calculation quantity: %d and allocation quantity: %d mismatch, pod: %s container: %s",
							numaId, numaCalculationQuantities[numaId], cset.Size(), podUID, containerName)
					}
				}

				for numaId, calQuantity := range numaCalculationQuantities {
					if calQuantity != allocationInfo.TopologyAwareAssignments[numaId].Size() {
						return fmt.Errorf("NUMA: %d calculation quantity: %d and allocation quantity: %d mismatch, pod: %s container: %s",
							numaId, calQuantity, allocationInfo.TopologyAwareAssignments[numaId].Size(), podUID, containerName)
					}
				}
			} else {
				calculationQuantity, err := GetCalculationInfoTotalQuantity(calculationInfo)

				if err != nil {
					return fmt.Errorf("GetCalculationInfoTotalQuantity failed with error: %v, pod: %s container: %s",
						err, podUID, containerName)
				}

				// currently we don't support strategy to adjust cpuset of dedicated_cores containers.
				// for stability if the dedicated_cores container calculation result and allocation result,
				// we will return error.
				if calculationQuantity != allocationInfo.AllocationResult.Size() {
					return fmt.Errorf("pod: %s container: %s calculation result: %d and allocation result: %s mismatch",
						podUID, containerName, calculationQuantity, allocationInfo.AllocationResult.Size())
				}
			}
		}
	}

	return nil
}

func validateStaticPools(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {
	for _, poolName := range StaticPools.List() {

		var nilStateEntry, nilRespEntry bool
		if entries[poolName] == nil || entries[poolName][""] == nil {
			nilStateEntry = true
		}

		if resp.Entries[poolName] == nil || resp.Entries[poolName].Entries[""] == nil {
			nilRespEntry = true
		}

		if nilStateEntry != nilRespEntry {
			return fmt.Errorf("pool: %s nilStateEntry: %v and nilRespEntry: %v mismatch",
				poolName, nilStateEntry, nilStateEntry)
		}

		if nilStateEntry {
			klog.Warningf("[validateStaticPools] got nil state entry for static pool: %s", poolName)
			continue
		}

		allocationInfo := entries[poolName][""]
		calculationInfo := resp.Entries[poolName].Entries[""]

		if calculationInfo.OwnerPoolName != poolName {
			return fmt.Errorf("pool: %s has invalid owner pool name: %s in cpu advisor resp",
				poolName, calculationInfo.OwnerPoolName)
		}

		if len(calculationInfo.CalculationResultsByNumas) != 1 ||
			calculationInfo.CalculationResultsByNumas[-1] == nil ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 {
			return fmt.Errorf("static pool: %s has invalid calculationInfo", poolName)
		}

		calculationQuantity, err := GetCalculationInfoTotalQuantity(calculationInfo)

		if err != nil {
			return fmt.Errorf("GetCalculationInfoTotalQuantity failed with error: %v, pool: %s",
				err, poolName)
		}

		// currently we don't support strategy to adjust cpuset of static pools.
		// for stability if the static pool calculation result and allocation result,
		// we will return error.
		if calculationQuantity != allocationInfo.AllocationResult.Size() {
			return fmt.Errorf("static pool: %s calculation result: %d and allocation result: %s mismatch",
				poolName, calculationQuantity, allocationInfo.AllocationResult.Size())
		}
	}

	return nil
}

func ValidateCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {

	if resp == nil {
		return fmt.Errorf("got nil cpu advisor resp")
	}

	var errList []error

	for _, validator := range listAndWatchResponseValidators {
		errList = append(errList, validator(entries, resp))
	}

	return errors.NewAggregate(errList)
}

func FilterDedicatedCalculationInfos(resp *advisorapi.ListAndWatchResponse) map[string]map[string]*advisorapi.CalculationInfo {
	dedicatedCalculationInfos := make(map[string]map[string]*advisorapi.CalculationInfo)

	for entryName, entry := range resp.Entries {
		for subEntryName, calculationInfo := range entry.Entries {
			if calculationInfo != nil && calculationInfo.OwnerPoolName == PoolNameDedicated {

				if dedicatedCalculationInfos[entryName] == nil {
					dedicatedCalculationInfos[entryName] = make(map[string]*advisorapi.CalculationInfo)
				}

				dedicatedCalculationInfos[entryName][subEntryName] = calculationInfo
			}
		}
	}

	return dedicatedCalculationInfos
}

func FilterDedicatedAllocationInfos(entries PodEntries) PodEntries {
	numaBindingEntries := make(PodEntries)

	for podUID, containerEntries := range entries {
		if containerEntries.IsPoolEntry() {
			continue
		}

		for containerName, allocationInfo := range containerEntries {
			if allocationInfo != nil &&
				allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores {

				if numaBindingEntries[podUID] == nil {
					numaBindingEntries[podUID] = make(ContainerEntries)
				}

				numaBindingEntries[podUID][containerName] = allocationInfo.Clone()
			}
		}
	}

	return numaBindingEntries
}

func GetCalculationInfoNUMAQuantities(calculationInfo *advisorapi.CalculationInfo) (map[int]int, error) {
	if calculationInfo == nil {
		return nil, fmt.Errorf("got nil calculationInfo")
	}

	numaQuantities := make(map[int]int)
	for numaId, numaResult := range calculationInfo.CalculationResultsByNumas {
		if numaResult == nil {
			klog.Warningf("[GetCalculationInfoTotalQuantity] got nil NUMA result")
			continue
		}

		var quantityUInt64 uint64 = 0
		for _, block := range numaResult.Blocks {
			if block == nil {
				klog.Warningf("[GetCalculationInfoTotalQuantity] got nil block")
				continue
			}

			quantityUInt64 += block.Result
		}

		quantityInt, err := general.CovertUInt64ToInt(quantityUInt64)

		if err != nil {
			return nil, fmt.Errorf("converting quantity: %s to int failed with error: %v",
				quantityUInt64, err)
		}

		numaIdInt, err := general.CovertInt64ToInt(numaId)

		if err != nil {
			return nil, fmt.Errorf("converting quantity: %s to int failed with error: %v",
				numaId, err)
		}

		numaQuantities[numaIdInt] = quantityInt
	}

	return numaQuantities, nil
}

func GetCalculationInfoTotalQuantity(calculationInfo *advisorapi.CalculationInfo) (int, error) {
	if calculationInfo == nil {
		return 0, fmt.Errorf("got nil calculationInfo")
	}

	var quantityUInt64 uint64 = 0
	for _, numaResult := range calculationInfo.CalculationResultsByNumas {
		if numaResult == nil {
			klog.Warningf("[GetCalculationInfoTotalQuantity] got nil NUMA result")
			continue
		}

		for _, block := range numaResult.Blocks {
			if block == nil {
				klog.Warningf("[GetCalculationInfoTotalQuantity] got nil block")
				continue
			}

			quantityUInt64 += block.Result
		}
	}

	quantityInt, err := general.CovertUInt64ToInt(quantityUInt64)

	if err != nil {
		return 0, fmt.Errorf("converting quantity: %s to int failed with error: %v",
			quantityUInt64, err)
	}

	return quantityInt, nil
}
