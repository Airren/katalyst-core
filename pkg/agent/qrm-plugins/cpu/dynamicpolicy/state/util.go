/*
Copyright 2022 The Katalyst Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package state

import (
	"fmt"

	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/klog/v2"

	"github.com/kubewharf/katalyst-api/pkg/consts"
	advisorapi "github.com/kubewharf/katalyst-core/pkg/agent/qrm-plugins/cpu/dynamicpolicy/cpuadvisor"
	"github.com/kubewharf/katalyst-core/pkg/util/machine"
)

const (
	PoolNameShare     = "share"
	PoolNameReclaim   = "reclaim"
	PoolNameDedicated = "dedicated"
	PoolNameReserve   = "reserve"
	// PoolNameFallback is not a real pool and is the union result of
	// all online pools to put pod should have been isolated
	PoolNameFallback = "fallback"
)

var (
	// StaticPools are generated by cpu plugin statically,
	// and they will be ignored when reading cpu advisor list and watch response
	StaticPools = sets.NewString(
		PoolNameReserve,
	)

	// ResidentPools are guaranteed existing in state
	ResidentPools = sets.NewString(
		PoolNameReclaim,
	).Union(StaticPools)
)

var GetContainerRequestedCores func(allocationInfo *AllocationInfo) int

// GetPoolsQuantityMapFromCPUAdvisorResp GetQuantityMap returns a map from cpu-set pool to cpu size;
// this info is constructed by advisor response, and this response contain cpu-set pool
// by a special container name .i.e empty string
func GetPoolsQuantityMapFromCPUAdvisorResp(resp *advisorapi.ListAndWatchResponse) (map[string]int, error) {
	ret := make(map[string]int)

	if resp == nil {
		return ret, nil
	}

	for poolName, entry := range resp.Entries {
		calculationEntries := entry.Entries

		// only deal with pool
		if len(calculationEntries) != 1 || calculationEntries[""] == nil {
			continue
		}

		calculationInfo := calculationEntries[""]

		// todo: currently sys advisor only returns calculation results
		//  for pools which are npt overlapped with others and isn't NUMA aware.
		// 	and we should refine judgement here when sys advisor supports NUMA
		// 	aware pool results overlapped with others.
		if calculationInfo == nil ||
			len(calculationInfo.CalculationResultsByNumas) != 1 ||
			calculationInfo.CalculationResultsByNumas[-1] == nil ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].OverlapTargets) != 0 ||
			calculationInfo.OwnerPoolName != poolName {
			return nil, fmt.Errorf("invalid calculation result: %s for pool: %s", calculationInfo.String(), poolName)
		}

		if StaticPools.Has(poolName) {
			klog.Infof("[GetPoolsQuantityMapFromCPUAdvisorResp] skip static pool: %s in cpu advisor resp", poolName)
			continue
		}

		ret[poolName] = int(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].Result)
	}
	return ret, nil
}

// GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp returns a map from cpu-set pool to cpu size;
// this info is constructed by combine advisor response and given pod entry list
func GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) (map[string]int, error) {
	entriesQuantityMap := machine.GetQuantityMap(entries.GetPoolsCPUset(ResidentPools))

	qosAwarePoolsQuantityMap, err := GetPoolsQuantityMapFromCPUAdvisorResp(resp)
	if err != nil {
		return nil, fmt.Errorf("GetPoolsQuantityMapFromCPUAdvisorResp failed with error: %v", err)
	}

	poolsQuantityMap := make(map[string]int)
	for poolName, quantity := range entriesQuantityMap {
		poolsQuantityMap[poolName] = quantity

		if _, found := qosAwarePoolsQuantityMap[poolName]; !found {
			klog.Warningf("[GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp] pool: %s isn't found in qos aware resp", poolName)
		}
	}

	// overwrite entriesQuantityMap by qosAwarePoolsQuantityMap
	for poolName, quantity := range qosAwarePoolsQuantityMap {
		if prevQuantity, found := poolsQuantityMap[poolName]; found && prevQuantity != quantity {
			klog.Infof("[GetPoolsQuantityMapFromPodEntriesAndCPUAdvisorResp] pool: %s overwrite quantity: %d by quantity: %d provided by qos aware",
				poolName, prevQuantity, quantity)
		}
		poolsQuantityMap[poolName] = quantity
	}

	return poolsQuantityMap, nil
}

func GetIsolatedQuantityMapFromPodEntriesAndCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) (map[string]map[string]int, error) {
	dedicatedCoresIsolatedQuantityMap := GetIsolatedQuantityMapFromPodEntries(entries, nil)
	qosAwareIsolatedQuantityMap, err := GetIsolatedQuantityMapFromCPUAdvisorResp(resp, entries)
	if err != nil {
		return nil, fmt.Errorf("GetIsolatedQuantityMapFromCPUAdvisorResp failed with error: %v", err)
	}

	isolatedQuantityMap := make(map[string]map[string]int)
	for podUID, containerEntries := range dedicatedCoresIsolatedQuantityMap {
		if isolatedQuantityMap[podUID] == nil {
			isolatedQuantityMap[podUID] = make(map[string]int)
		}

		for containerName, quantity := range containerEntries {
			isolatedQuantityMap[podUID][containerName] = quantity
		}
	}

	// overwrite dedicatedCoresIsolatedQuantityMap by qosAwareIsolatedQuantityMap
	for podUID, containerEntries := range qosAwareIsolatedQuantityMap {
		if isolatedQuantityMap[podUID] == nil {
			isolatedQuantityMap[podUID] = make(map[string]int)
		}

		for containerName, quantity := range containerEntries {
			if prevQuantity, found := isolatedQuantityMap[podUID][containerName]; found && prevQuantity != quantity {
				klog.Infof("[GetIsolatedQuantityMapFromPodEntriesAndCPUAdvisorResp] pod: %s, container: %s overwrite quantity: %d by quantity: %d provided by qos aware",
					podUID, containerName, prevQuantity, quantity)
			}
			isolatedQuantityMap[podUID][containerName] = quantity
		}
	}

	return isolatedQuantityMap, nil
}

func GetIsolatedQuantityMapFromCPUAdvisorResp(resp *advisorapi.ListAndWatchResponse, chkEntries PodEntries) (map[string]map[string]int, error) {
	ret := make(map[string]map[string]int)

	if resp == nil {
		return ret, nil
	}

	for podUID, entry := range resp.Entries {
		calculationEntries := entry.Entries

		// not deal with pool
		if len(calculationEntries) == 1 && calculationEntries[""] != nil {
			continue
		}

		for containerName, calculationInfo := range calculationEntries {
			if calculationInfo == nil ||
				calculationInfo.OwnerPoolName != PoolNameDedicated {
				continue
			}

			allocationInfo := chkEntries[podUID][containerName]

			if allocationInfo == nil {
				klog.Warningf("[GetIsolatedQuantityMapFromCPUAdvisorResp] qos aware server get entry for pod: %s, container: %s which isn't in checkpoint", podUID, containerName)
				continue
			} else if allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
				continue
			}

			// todo: currently not support per NUMA calculation results for dedicated_cores without numa_binding
			if len(calculationInfo.CalculationResultsByNumas) != 1 ||
				calculationInfo.CalculationResultsByNumas[-1] == nil ||
				len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 {
				return nil, fmt.Errorf("invalid calculation result for pod: %s/%s, container: %s, owner pool: %s",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName, calculationInfo.OwnerPoolName)
			}

			quantity := int(calculationInfo.CalculationResultsByNumas[-1].Blocks[0].Result)

			if quantity == 0 {
				klog.Warningf("[GetIsolatedQuantityMapFromCPUAdvisorResp] dedicated_cores pod: %s/%s container: %s get zero quantity",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName)
				continue
			}

			if ret[podUID] == nil {
				ret[podUID] = make(map[string]int)
			}
			ret[podUID][containerName] = quantity
		}
	}

	return ret, nil
}

func GetIsolatedQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]map[string]int {
	ret := make(map[string]map[string]int)

	for podUID, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for containerName, allocationInfo := range entries {
			// only filter dedicated_cores without numa_binding
			if allocationInfo == nil ||
				// dedicated_cores with numa_binding
				(allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
					allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable) ||
				(allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelDedicatedCores) {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding
			// to pool rather than isolation. calling this function means we will start to adjust allocation,
			// and we will try to isolate those containers, so we will treat them as containers to be isolated.
			var quantity int
			if allocationInfo.OwnerPoolName != PoolNameDedicated {
				quantity = GetContainerRequestedCores(allocationInfo)
			} else {
				quantity = allocationInfo.AllocationResult.Size()
			}

			if quantity == 0 {
				klog.Warningf("[GetIsolatedQuantityMapFromPodEntries] isolated pod: %s/%s container: %s get zero quantity",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName)
				continue
			}

			if ret[podUID] == nil {
				ret[podUID] = make(map[string]int)
			}
			ret[podUID][containerName] = quantity
		}
	}

	return ret
}

// GetPoolsQuantityMapFromPodEntries is used to constrict mapping from
// pool name to its corresponding resource quantity
func GetPoolsQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]int {
	ret := make(map[string]int)

	for _, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for _, allocationInfo := range entries {
			// only count shared_cores not isolated.
			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding to pool rather than isolation.
			// calling this function means we will start to adjust allocation and we will try to isolate those containers,
			// so we will treat them as containers to be isolated.
			if allocationInfo == nil || allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelSharedCores {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			poolName := GetRealOwnerPoolName(allocationInfo)
			if poolName != "" {
				ret[poolName] += GetContainerRequestedCores(allocationInfo)
			}
		}
	}

	return ret
}

// GetSpecifiedPoolName parses the belonging pool name for a given allocation according to QoS level
func GetSpecifiedPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	switch allocationInfo.QoSLevel {
	case consts.PodAnnotationQoSLevelSharedCores:
		specifiedPoolName := GetSpecifiedPoolNameForSharedCores(allocationInfo)
		if specifiedPoolName != "" {
			return specifiedPoolName
		}
		return PoolNameShare
	case consts.PodAnnotationQoSLevelReclaimedCores:
		return PoolNameReclaim
	default:
		return ""
	}
}

// GetRealOwnerPoolName parses the owner pool name for a given allocation
func GetRealOwnerPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.OwnerPoolName
}

// GetSpecifiedPoolNameForSharedCores returns cpu enhancement from allocation
func GetSpecifiedPoolNameForSharedCores(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.Annotations[consts.PodAnnotationCPUEnhancementCPUSet]
}

// GenerateCPUMachineStateByPodEntries is used to re-organize podEntries
// by assembling allocation info into each NUMA node
func GenerateCPUMachineStateByPodEntries(topology *machine.CPUTopology, podEntries PodEntries) (NUMANodeMap, error) {
	if topology == nil {
		return nil, fmt.Errorf("GenerateCPUMachineStateByPodEntries got nil topology")
	}

	machineState := make(NUMANodeMap)
	for _, numaNode := range topology.CPUDetails.NUMANodes().ToSliceInt64() {
		numaNodeState := &NUMANodeState{}

		numaNodeAllCPUs := topology.CPUDetails.CPUsInNUMANodes(int(numaNode)).Clone()
		allocatedCPUsInNumaNode := machine.NewCPUSet()

		for podUID, containerEntries := range podEntries {
			for containerName, allocationInfo := range containerEntries {
				if containerName != "" && allocationInfo != nil {

					// the container hasn't cpuset assignment in the current NUMA node
					if allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)].Size() == 0 &&
						allocationInfo.TopologyAwareAssignments[int(numaNode)].Size() == 0 {
						continue
					}

					// only modify allocated and default properties in NUMA node state for dedicated_cores with NUMA binding
					if allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
						allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
						// only consider original in machine state
						allocatedCPUsInNumaNode = allocatedCPUsInNumaNode.Union(allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)])
					}

					numaNodeAllocationInfo := allocationInfo.Clone()

					topologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs))
					originalTopologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs))

					numaNodeAllocationInfo.AllocationResult = allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.OriginalAllocationResult = allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.TopologyAwareAssignments = topologyAwareAssignments
					numaNodeAllocationInfo.OriginalTopologyAwareAssignments = originalTopologyAwareAssignments

					numaNodeState.SetAllocationInfo(podUID, containerName, numaNodeAllocationInfo)
				}
			}
		}

		numaNodeState.AllocatedCPUSet = allocatedCPUsInNumaNode.Clone()
		numaNodeState.DefaultCPUSet = numaNodeAllCPUs.Difference(numaNodeState.AllocatedCPUSet)

		machineState[int(numaNode)] = numaNodeState
	}

	return machineState, nil
}

func UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp(chkEntries PodEntries, resp *advisorapi.ListAndWatchResponse, poolsQuantityMap map[string]int) error {
	if resp == nil {
		return nil
	}

	for podUID, entry := range resp.Entries {

		calculationEntries := entry.Entries

		// not deal with pool
		if len(calculationEntries) == 1 && calculationEntries[""] != nil {
			continue
		}

		for containerName, calculationInfo := range calculationEntries {
			if calculationInfo == nil ||
				calculationInfo.OwnerPoolName == "" ||
				calculationInfo.OwnerPoolName == PoolNameDedicated {
				continue
			}

			allocationInfo := chkEntries[podUID][containerName]

			if allocationInfo == nil {
				klog.Warningf("[UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp] qos aware server get entry for pod: %s, container: %s which isn't in checkpoint", podUID, containerName)
				continue
			} else if allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelSharedCores && allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelReclaimedCores {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with invalid ownerPoolName: %s",
					podUID, containerName, allocationInfo.QoSLevel, calculationInfo.OwnerPoolName)
			} else if calculationInfo.CalculationResultsByNumas != nil {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with non-empty CalculationResultsByNumas",
					podUID, containerName, allocationInfo.QoSLevel)
			} else if _, found := poolsQuantityMap[calculationInfo.OwnerPoolName]; !found {
				return fmt.Errorf("qos aware server return entry for pod: %s, container: %s, qosLevel: %s with non-empty CalculationResultsByNumas",
					podUID, containerName, allocationInfo.QoSLevel)
			}

			if allocationInfo.OwnerPoolName != calculationInfo.OwnerPoolName {
				klog.Infof("[UpdateOwnerPoolsForSharedCoresByCPUAdvisorResp] qos aware server put pod: %s/%s, container: %s from %s to %s", allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName, allocationInfo.OwnerPoolName, calculationInfo.OwnerPoolName)
			}
			// set allocationInfo.OwnerPoolName as qos aware sever indicates
			allocationInfo.OwnerPoolName = calculationInfo.OwnerPoolName
		}
	}

	return nil
}
